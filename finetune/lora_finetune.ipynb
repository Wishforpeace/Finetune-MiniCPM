{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniCPM-2B 参数高效微调（LoRA）A100 80G 单卡示例\n",
    "\n",
    "显存更小的显卡可用 batch size 和 grad_accum 间时间换空间\n",
    "\n",
    "本 notebook 是一个使用 `AdvertiseGen` 数据集对 MiniCPM-2B 进行 LoRA 微调，使其具备专业的广告生成能力的代码示例。\n",
    "\n",
    "## 最低硬件需求\n",
    "- 显存：12GB\n",
    "- 显卡架构：安培架构（推荐）\n",
    "- 内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 准备数据集\n",
    "\n",
    "将数据集转换为更通用的格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为 ChatML 格式\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "input_dir = \"data/AdvertiseGen\"\n",
    "output_dir = \"data/AdvertiseGenChatML\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for fn in [\"train.json\", \"dev.json\"]:\n",
    "    data_out_list = []\n",
    "    with open(os.path.join(input_dir, fn), \"r\") as f, open(os.path.join(output_dir, fn), \"w\") as fo:\n",
    "        for line in f:\n",
    "            if len(line.strip()) > 0:\n",
    "                data = json.loads(line)\n",
    "                data_out = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": data[\"content\"],\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": data[\"summary\"],\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "                data_out_list.append(data_out)\n",
    "        json.dump(data_out_list, fo, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用 LoRA 进行微调\n",
    "\n",
    "命令行一键运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240515142253\n",
      "[2024-05-15 14:22:55,122] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 14:22:56,390] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-05-15 14:22:56,401] [INFO] [runner.py:568:main] cmd = /home/mnt/wyx/anaconda3/envs/finetune/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None finetune.py --model_name_or_path /home/mnt/wyx/src/Finetune-MiniCPM/MiniCPM-2B-sft-bf16 --output_dir output/AdvertiseGenLoRA/20240515142253/ --train_data_path data/AdvertiseGenChatML/train.json --eval_data_path data/AdvertiseGenChatML/dev.json --learning_rate 5e-5 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --model_max_length 384 --bf16 --use_lora --gradient_accumulation_steps 1 --warmup_steps 100 --max_steps 3000 --weight_decay 0.01 --evaluation_strategy steps --eval_steps 500 --save_strategy steps --save_steps 500 --seed 42 --log_level info --logging_strategy steps --logging_steps 10 --deepspeed configs/ds_config_zero3_offload.json\n",
      "[2024-05-15 14:22:57,877] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 14:22:59,104] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2024-05-15 14:22:59,104] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2024-05-15 14:22:59,104] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2024-05-15 14:22:59,104] [INFO] [launch.py:163:main] dist_world_size=1\n",
      "[2024-05-15 14:22:59,104] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "[2024-05-15 14:22:59,105] [INFO] [launch.py:253:main] process 366842 spawned with command: ['/home/mnt/wyx/anaconda3/envs/finetune/bin/python', '-u', 'finetune.py', '--local_rank=0', '--model_name_or_path', '/home/mnt/wyx/src/Finetune-MiniCPM/MiniCPM-2B-sft-bf16', '--output_dir', 'output/AdvertiseGenLoRA/20240515142253/', '--train_data_path', 'data/AdvertiseGenChatML/train.json', '--eval_data_path', 'data/AdvertiseGenChatML/dev.json', '--learning_rate', '5e-5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '2', '--model_max_length', '384', '--bf16', '--use_lora', '--gradient_accumulation_steps', '1', '--warmup_steps', '100', '--max_steps', '3000', '--weight_decay', '0.01', '--evaluation_strategy', 'steps', '--eval_steps', '500', '--save_strategy', 'steps', '--save_steps', '500', '--seed', '42', '--log_level', 'info', '--logging_strategy', 'steps', '--logging_steps', '10', '--deepspeed', 'configs/ds_config_zero3_offload.json']\n",
      "[2024-05-15 14:23:01,404] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-15 14:23:02,912] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-15 14:23:02,912] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-15 14:23:13,619] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 363, num_elems = 3.01B\n",
      "trainable params: 2,949,120 || all params: 2,727,830,016 || trainable%: 0.10811230841738784\n",
      "input: <s> <用户> 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤 <AI> 宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "label: 宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。</s>\n",
      "input: <s> <用户> 类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞 <AI> 简约而不简单的牛仔外套，白色的衣身十分百搭。衣身多处有做旧破洞设计，打破单调乏味，增加一丝造型看点。衣身后背处有趣味刺绣装饰，丰富层次感，彰显别样时尚。</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "label: 简约而不简单的牛仔外套，白色的衣身十分百搭。衣身多处有做旧破洞设计，打破单调乏味，增加一丝造型看点。衣身后背处有趣味刺绣装饰，丰富层次感，彰显别样时尚。</s>\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/mnt/wyx/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/mnt/wyx/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.395176887512207 seconds\n",
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2024-05-15 14:23:23,025] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-15 14:23:23,047] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-15 14:23:23,049] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-05-15 14:23:23,049] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-15 14:23:23,060] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-05-15 14:23:23,060] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-05-15 14:23:23,060] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-05-15 14:23:23,060] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\n",
      "[2024-05-15 14:23:23,225] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-05-15 14:23:23,225] [INFO] [utils.py:801:see_memory_usage] MA 0.04 GB         Max_MA 1.13 GB         CA 0.09 GB         Max_CA 1 GB \n",
      "[2024-05-15 14:23:23,225] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.27 GB, percent = 18.5%\n",
      "[2024-05-15 14:23:23,231] [INFO] [stage3.py:130:__init__] Reduce bucket size 5308416\n",
      "[2024-05-15 14:23:23,231] [INFO] [stage3.py:131:__init__] Prefetch bucket size 4777574\n",
      "[2024-05-15 14:23:23,383] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-05-15 14:23:23,384] [INFO] [utils.py:801:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2024-05-15 14:23:23,384] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.27 GB, percent = 18.5%\n",
      "Parameter Offload: Total persistent parameters: 3135744 in 241 params\n",
      "[2024-05-15 14:23:23,637] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-05-15 14:23:23,637] [INFO] [utils.py:801:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2024-05-15 14:23:23,638] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.28 GB, percent = 18.5%\n",
      "[2024-05-15 14:23:23,799] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-05-15 14:23:23,800] [INFO] [utils.py:801:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2024-05-15 14:23:23,800] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.28 GB, percent = 18.5%\n",
      "[2024-05-15 14:23:23,972] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2024-05-15 14:23:23,973] [INFO] [utils.py:801:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2024-05-15 14:23:23,973] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.29 GB, percent = 18.6%\n",
      "[2024-05-15 14:23:24,134] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-05-15 14:23:24,135] [INFO] [utils.py:801:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2024-05-15 14:23:24,135] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.29 GB, percent = 18.6%\n",
      "[2024-05-15 14:23:24,307] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions\n",
      "[2024-05-15 14:23:24,307] [INFO] [utils.py:801:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2024-05-15 14:23:24,307] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.3 GB, percent = 18.6%\n",
      "[2024-05-15 14:23:24,468] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-15 14:23:24,469] [INFO] [utils.py:801:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2024-05-15 14:23:24,469] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.3 GB, percent = 18.6%\n",
      "[2024-05-15 14:23:24,651] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-15 14:23:24,652] [INFO] [utils.py:801:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2024-05-15 14:23:24,652] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.32 GB, percent = 18.6%\n",
      "[2024-05-15 14:23:24,652] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2024-05-15 14:23:24,863] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-15 14:23:24,864] [INFO] [utils.py:801:see_memory_usage] MA 0.05 GB         Max_MA 0.05 GB         CA 0.1 GB         Max_CA 0 GB \n",
      "[2024-05-15 14:23:24,864] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.33 GB, percent = 18.6%\n",
      "[2024-05-15 14:23:24,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\n",
      "[2024-05-15 14:23:24,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-05-15 14:23:24,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-15 14:23:24,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2024-05-15 14:23:24,867] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-15 14:23:24,867] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-15 14:23:24,867] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-15 14:23:24,867] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-15 14:23:24,867] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-15 14:23:24,867] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc3500cf160>\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-05-15 14:23:24,868] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   train_batch_size ............. 2\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=5308416 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=4777574 param_persistence_threshold=23040 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3\n",
      "[2024-05-15 14:23:24,869] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_bucket_size\": 5.308416e+06, \n",
      "        \"stage3_prefetch_bucket_size\": 4.777574e+06, \n",
      "        \"stage3_param_persistence_threshold\": 2.304000e+04, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"train_batch_size\": 2, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"flops_profiler\": {\n",
      "        \"enabled\": false, \n",
      "        \"profile_step\": 1, \n",
      "        \"module_depth\": -1, \n",
      "        \"top_modules\": 1, \n",
      "        \"detailed\": true, \n",
      "        \"output_file\": null\n",
      "    }, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 2,949,120\n",
      "{'loss': 3.6619, 'grad_norm': 4.123636656212685, 'learning_rate': 5e-06, 'epoch': 0.0}\n",
      "{'loss': 4.0214, 'grad_norm': 5.091009608118615, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7903, 'grad_norm': 3.702435908203937, 'learning_rate': 1.5e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4913, 'grad_norm': 4.240356049972926, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5547, 'grad_norm': 3.093571359119955, 'learning_rate': 2.5e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5985, 'grad_norm': 3.3507752105167508, 'learning_rate': 3e-05, 'epoch': 0.0}\n",
      "{'loss': 3.659, 'grad_norm': 3.5394862176072035, 'learning_rate': 3.5e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3005, 'grad_norm': 3.1927137929715164, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5751, 'grad_norm': 3.516425825673475, 'learning_rate': 4.5e-05, 'epoch': 0.0}\n",
      "{'loss': 3.489, 'grad_norm': 2.8472799672626325, 'learning_rate': 5e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3985, 'grad_norm': 3.1452570675827434, 'learning_rate': 4.982758620689655e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4681, 'grad_norm': 3.1761675572720773, 'learning_rate': 4.9655172413793107e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3691, 'grad_norm': 3.7352955214341863, 'learning_rate': 4.9482758620689655e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4033, 'grad_norm': 3.5589556546643957, 'learning_rate': 4.931034482758621e-05, 'epoch': 0.0}\n",
      "{'loss': 3.309, 'grad_norm': 3.201023510327888, 'learning_rate': 4.913793103448276e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2964, 'grad_norm': 3.086384955692754, 'learning_rate': 4.896551724137931e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3009, 'grad_norm': 3.8366541921333814, 'learning_rate': 4.8793103448275864e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2765, 'grad_norm': 3.53149345706494, 'learning_rate': 4.862068965517241e-05, 'epoch': 0.0}\n",
      "  6%|██▍                                     | 187/3000 [03:46<55:43,  1.19s/it][2024-05-15 14:27:15,384] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 366842\n",
      "[2024-05-15 14:27:15,384] [ERROR] [launch.py:322:sigkill_handler] ['/home/mnt/wyx/anaconda3/envs/finetune/bin/python', '-u', 'finetune.py', '--local_rank=0', '--model_name_or_path', '/home/mnt/wyx/src/Finetune-MiniCPM/MiniCPM-2B-sft-bf16', '--output_dir', 'output/AdvertiseGenLoRA/20240515142253/', '--train_data_path', 'data/AdvertiseGenChatML/train.json', '--eval_data_path', 'data/AdvertiseGenChatML/dev.json', '--learning_rate', '5e-5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '2', '--model_max_length', '384', '--bf16', '--use_lora', '--gradient_accumulation_steps', '1', '--warmup_steps', '100', '--max_steps', '3000', '--weight_decay', '0.01', '--evaluation_strategy', 'steps', '--eval_steps', '500', '--save_strategy', 'steps', '--save_steps', '500', '--seed', '42', '--log_level', 'info', '--logging_strategy', 'steps', '--logging_steps', '10', '--deepspeed', 'configs/ds_config_zero3_offload.json'] exits with return code = -9\n"
     ]
    }
   ],
   "source": [
    "!bash lora_finetune.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/mnt/wyx/src/Finetune-MiniCPM/finetune/output/AdvertiseGenLoRA/20240515145527 does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/mnt/wyx/src/Finetune-MiniCPM/finetune/output/AdvertiseGenLoRA/20240515145527/tree/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mnt/wyx/src/Finetune-MiniCPM/finetune/output/AdvertiseGenLoRA/20240515145527\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     path, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finetune/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:819\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 819\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m~/anaconda3/envs/finetune/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:928\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    926\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 928\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    930\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/anaconda3/envs/finetune/lib/python3.10/site-packages/transformers/configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    633\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/finetune/lib/python3.10/site-packages/transformers/configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/finetune/lib/python3.10/site-packages/transformers/utils/hub.py:369\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 369\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/mnt/wyx/src/Finetune-MiniCPM/finetune/output/AdvertiseGenLoRA/20240515145527 does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/mnt/wyx/src/Finetune-MiniCPM/finetune/output/AdvertiseGenLoRA/20240515145527/tree/None' for available files."
     ]
    }
   ],
   "source": [
    "path = \"output/AdvertiseGenLoRA/20240315224356/checkpoint-3000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path, torch_dtype=torch.bfloat16, device_map=\"cuda\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, history = model.chat(tokenizer, query=\"<用户>类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞<AI>\", max_length=80, top_p=0.5)\n",
    "res, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你已经加载了模型和tokenizer\n",
    "model_path = \"output/AdvertiseGenLoRA/20240515152537/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('这款牛仔外套，采用白色牛仔布面料，简约的白色，更显干净利落。破洞的装饰，增加了几分时尚感，同时又不失个性。简约的',\n",
       " [{'role': 'user',\n",
       "   'content': '<用户>类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞<AI>'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '这款牛仔外套，采用白色牛仔布面料，简约的白色，更显干净利落。破洞的装饰，增加了几分时尚感，同时又不失个性。简约的'}])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, history = model.chat(tokenizer, query=\"<用户>类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞<AI>\", max_length=80, top_p=0.5)\n",
    "res, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
